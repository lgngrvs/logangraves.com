# In expectation of a changing world
Date: 2024-07-26
Tags: ai
Type: post
Desc: Considering what the world looks like when I graduate

General intelligence before the end of the decade seems reasonable. If I am going to *actually believe what I say,* I need to spend some more time thinking about my place in that world.

I think general intelligence is plausible because multimodal AI already works *reasonably well.* Really good multimodal AI and a good agent architecture is probably enough to approximate AGI to the point where intelligence explosion starts to become possible — when agents are good enough to start accelerating AI research. AI research is, after all, one of the first things that gets automated with better AI. It's work that can be done remotely: digest a codebase, attend check-in meetings, ask questions, submit code; these all seem like things AI already knows how to do, just *not well enough* and *not unified into a single agent yet* (mostly). Simply getting more reliable models that do things that we already know how to make AI do gets us some part of the way. AI researchers also know what AI research looks like — and there are strong economic incentives to automate AI research; faster innovation means your lab gets an edge against your competitors. Wherever you can get an edge matters — and the field's greatest minds will be working on these problems.

Obviously, AGI is not *here.* Yet. We'll need technical breakthroughs to get there. We'll have to get past data constraints (at least, within certain modalities like text), we'll need significantly more reliability. We'll need models that are just *smarter* — that *just work.* We'll need models that have long-term rational planning abilities, that, probably through being embedded in the right cognitive architecture, can interact with the world with agency. We'll also need a *significant* industrial buildout to continue [scaling what works.]\(==more dukka== \) — power, chips, security, will all need to scale up massively, and quickly.

This is all to say nothing about getting there *safely.* Interpretability, control, and alignment are probably harder problems than simply getting AGI — and the incentives are much less present to pursue them.

But the key idea here is that — though I'm not intimately familiar with the state of the art as of writing — looking at the way things are going now, and thinking about how things will likely go in the future, it doesn't take leaps of logic to say that AGI will be here, potentially within the decade. For me, an incoming college freshman as of writing, *potentially before I graduate college*. On vibes I think the most likely outcome is 6-8 years — but this is a very low-confidence, intuitive estimate, and I wouldn't gamble on it. Either way, I want to be in a position where I am prepared for the world to change, as it will.. 

**What is my place in this future? How does the world, my world, look?**

--- 

I'm graduating. It's 2028. The AGI race is on. This is plausibly the most important event of my lifetime. I want to do what I can to help it go better.

I can think of a couple ways to do this: 
1. Working from within the government or on its periphery, influencing policy. 
2. Being an actual politician.
3. Being actually involved in the organization and administration of *The Project* — building superintelligence.
4. Technical AI work — research on alignment and control. 
5. Technical non-AI work — infosec, chip physics and manufacturing, energy.

The third seems the most fun for me, and potentially naturally suited to who I am. My current estimate of my abilities is that one thing I'm abnormally good at (especially among AI-ish people is), for lack of a better-sounding word, leadership: I'm charismatic and effective at influencing people, bringing out emergent positive effects between them. With honing, this could be incredibly valuable.

I enjoy coding and technical work, but I think my counterfactual impact is probably highest in places that I'm naturally suited *and* uniquely impactful, and it seems like this is plausibly one of those areas — combining technical knowledge with organizational capability to make the people whose calling really is alignment successful.

Is this true? I'm medium-confidence on this conclusion. There's a pretty small number of alignment researchers, and to be honest I haven't tried that hard to "become cracked." I don't think I'm not intelligent enough — I guess we'll see how things go in college. Plausibly I just haven't found my niche yet.

I'm graduating college, and 
